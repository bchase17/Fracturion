{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import (fbeta_score, accuracy_score, f1_score, \n",
    "                             confusion_matrix, balanced_accuracy_score, recall_score, matthews_corrcoef, precision_score)\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "import gc\n",
    "pd.options.mode.chained_assignment = None \n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"A worker stopped while some jobs were given to the executor\",\n",
    "    category=UserWarning,\n",
    "    module=\"joblib.externals.loky.process_executor\"\n",
    ")\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import math\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_features(df):\n",
    "\n",
    "    df = df.sort_index(ascending=True)\n",
    "    \n",
    "    # =======================\n",
    "    # Basic SMAs and Ratios\n",
    "    # =======================\n",
    "    for window in [10, 25, 50, 100, 200]:\n",
    "        df[f'QQQ_SMA_{window}'] = df['Close'].rolling(window=window).mean()\n",
    "        df[f'QQQ_EMA_{window}'] = df['Close'].ewm(span=window, adjust=False).mean()\n",
    "        if window in (50, 100, 200):\n",
    "            df[f'num_days_{window}'] = 0\n",
    "\n",
    "    for window in [50, 100, 200]:\n",
    "        for i in range(1, len(df)):\n",
    "            prev = df.loc[i - 1, f'num_days_{window}']\n",
    "            price = df.loc[i, 'Close']\n",
    "            sma = df.loc[i, f'QQQ_SMA_{window}']\n",
    "            if price > sma:\n",
    "                df.loc[i, f'num_days_{window}'] = prev + 1 if prev >= 0 else 0\n",
    "            elif price < sma:\n",
    "                df.loc[i, f'num_days_{window}'] = prev - 1 if prev <= 0 else 0\n",
    "            else:\n",
    "                df.loc[i, f'num_days_{window}'] = 0\n",
    "\n",
    "        df[f'num_days_{window}'] = df[f'num_days_{window}'].apply(lambda x: int(5 * round(x / 5)))\n",
    "\n",
    "    # ============================\n",
    "    # Relative Position Features\n",
    "    # ============================\n",
    "    def rows_since_max(x): return len(x) - x.argmax() - 1\n",
    "    def rows_since_min(x): return len(x) - x.argmin() - 1\n",
    "\n",
    "    for window in [10, 30, 60, 120]:\n",
    "        df[f'Rel_Max_{window}'] = (df['High'] / df['High'].rolling(window=window).max()).round(3)\n",
    "        df[f'Rel_Min_{window}'] = (df['Low'] / df['Low'].rolling(window=window).min()).round(3)\n",
    "        df[f'Max_{window}_Rows_Since'] = df['High'].rolling(window=window).apply(rows_since_max, raw=True)\n",
    "        df[f'Min_{window}_Rows_Since'] = df['Low'].rolling(window=window).apply(rows_since_min, raw=True)\n",
    "\n",
    "    for a, b in [(50, 100), (50, 200), (100, 200), (10, 25), (10, 50), (10, 100), (10, 200), (25, 50), (25, 100), (25, 200)]:\n",
    "        df[f'{a}_SMA_{b}'] = (df[f'QQQ_SMA_{a}'] / df[f'QQQ_SMA_{b}']).round(3)\n",
    "        df[f'{a}_EMA_{b}'] = (df[f'QQQ_EMA_{a}'] / df[f'QQQ_EMA_{b}']).round(3)\n",
    "        df[f'{a}_ESMA_{b}'] = (df[f'QQQ_EMA_{a}'] / df[f'QQQ_SMA_{b}']).round(3)\n",
    "\n",
    "    for window in [10, 25, 50, 100, 200]:\n",
    "        df[f'QQQ_SMA_{window}'] = (df['Close'] / df[f'QQQ_SMA_{window}']).round(3)\n",
    "        df[f'QQQ_EMA_{window}'] = (df['Close'] / df[f'QQQ_EMA_{window}']).round(3)\n",
    "\n",
    "    # ================\n",
    "    # RSI Variants\n",
    "    # ================\n",
    "    def RSI(data, period):\n",
    "        delta = data['Close'].diff(1)\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "        RS = gain / loss\n",
    "        return (100 - (100 / (1 + RS))).round(0)\n",
    "\n",
    "    df['RSI_7'] = RSI(df, 7)\n",
    "    df['RSI_14'] = RSI(df, 14)\n",
    "    df['RSI_21'] = RSI(df, 21)\n",
    "\n",
    "    # ================\n",
    "    # MACD\n",
    "    # ================\n",
    "    ema_fast = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    ema_slow = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = (ema_fast - ema_slow).round(3)\n",
    "    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean().round(3)\n",
    "\n",
    "    # ================\n",
    "    # Bollinger Bands\n",
    "    # ================\n",
    "    bb_mid = df['Close'].rolling(window=20).mean()\n",
    "    bb_std = df['Close'].rolling(window=20).std()\n",
    "    df['BB_Upper'] = (bb_mid + 2 * bb_std).round(3) / df['Close']\n",
    "    df['BB_Lower'] = (bb_mid - 2 * bb_std).round(3) / df['Close']\n",
    "    df['BB_Mid'] = ((bb_mid * bb_std)/ df['Close']).round(3)\n",
    "\n",
    "    # ================\n",
    "    # VOLUME\n",
    "    # ================\n",
    "\n",
    "    # OBV Core\n",
    "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).fillna(0).cumsum()\n",
    "\n",
    "    # Momentum / Deviation\n",
    "    windows = [5, 10]\n",
    "    for w in windows:\n",
    "        df[f'OBV_ROC{w}'] = df['OBV'].pct_change(periods=w).round(3)\n",
    "        df[f'OBV_Z{w}'] = ((df['OBV'] - df['OBV'].rolling(w).mean()) / df['OBV'].rolling(w).std()).round(3)\n",
    "    \n",
    "    df['UpMask'] = df['Close'] > df['Close'].shift(1)\n",
    "    df['DownMask'] = df['Close'] < df['Close'].shift(1)\n",
    "    df['UpVolume'] = df['Volume'] * df['UpMask']\n",
    "    df['DownVolume'] = df['Volume'] * df['DownMask']\n",
    "    windows = [10, 25, 50, 100]\n",
    "    #df = calculate_obv_volume_ratio(df, windows)\n",
    "    for w in windows:\n",
    "        up = df['UpVolume'].rolling(w).sum()\n",
    "        down = df['DownVolume'].rolling(w).sum()\n",
    "\n",
    "        ratio = up / down.replace(0, np.nan)\n",
    "        ratio.fillna(1_000_000, inplace=True)  # Up-only\n",
    "        ratio[df['UpVolume'].rolling(w).sum() == 0] = 0  # Down-only\n",
    "\n",
    "        df[f'Vol_Ratio_{w}'] = ratio.round(3)\n",
    "\n",
    "    # Chaikin Money Flow (CMF)\n",
    "    def CMF(data, period=20):\n",
    "        mfm = ((data['Close'] - data['Low']) - (data['High'] - data['Close'])) / (data['High'] - data['Low'])\n",
    "        mfm = mfm.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "        mfv = mfm * data['Volume']\n",
    "        cmf = mfv.rolling(window=period).sum() / data['Volume'].rolling(window=period).sum()\n",
    "        return cmf.round(3)\n",
    "    \n",
    "    df['CMF_20'] = CMF(df, 20)\n",
    "    df['CMF_10'] = CMF(df, 10)\n",
    "     \n",
    "    # Volume Rate of Change (VROC)\n",
    "    windows = [3, 5, 10]\n",
    "    for w in windows:\n",
    "        df[f'VROC_{w}'] = df['Volume'].pct_change(periods=w).round(3)\n",
    "\n",
    "    # Normalized Volume Spike\n",
    "    windows = [10, 20, 40]\n",
    "    for w in windows:\n",
    "        df[f'Vol_Spike_{w}'] = (df['Volume'] / df['Volume'].rolling(w).median()).round(3)\n",
    "\n",
    "    # Accumulation/Distribution Line (ADL)\n",
    "    mfm = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / (df['High'] - df['Low'])\n",
    "    mfm = mfm.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    df['ADL'] = (mfm * df['Volume']).cumsum().round(3)\n",
    "\n",
    "    # ================\n",
    "    # ATR\n",
    "    # ================\n",
    "    tr = pd.concat([\n",
    "        df['High'] - df['Low'],\n",
    "        abs(df['High'] - df['Close'].shift()),\n",
    "        abs(df['Low'] - df['Close'].shift())\n",
    "    ], axis=1).max(axis=1)\n",
    "    windows = [7, 14, 21]\n",
    "    for w in windows:\n",
    "        df[f'ATR_{w}'] = tr.rolling(w).mean().round(1)\n",
    "\n",
    "    # ================\n",
    "    # ADX & DI\n",
    "    # ================\n",
    "    plus_dm = df['High'].diff()\n",
    "    minus_dm = df['Low'].diff()\n",
    "    plus_dm[plus_dm < 0] = 0\n",
    "    minus_dm[minus_dm > 0] = 0\n",
    "\n",
    "    tr = pd.concat([\n",
    "        df['High'] - df['Low'],\n",
    "        abs(df['High'] - df['Close'].shift(1)),\n",
    "        abs(df['Low'] - df['Close'].shift(1))\n",
    "    ], axis=1).max(axis=1)\n",
    "    atr = tr.rolling(14).mean()\n",
    "\n",
    "    plus_di = 100 * (plus_dm.ewm(alpha=1/14).mean() / atr)\n",
    "    minus_di = abs(100 * (minus_dm.ewm(alpha=1/14).mean() / atr))\n",
    "    dx = 100 * abs((plus_di - minus_di) / (plus_di + minus_di))\n",
    "    adx = dx.rolling(14).mean()\n",
    "\n",
    "    df['plus_DI'] = plus_di.round(0)\n",
    "    df['minus_DI'] = minus_di.round(0)\n",
    "    df['ADX'] = adx.round(0)\n",
    "\n",
    "    # ================\n",
    "    # Volatility\n",
    "    # ================\n",
    "    vol_5 = df['Close'].rolling(window=5).std().round(3)\n",
    "    vol_10 = df['Close'].rolling(window=10).std().round(3)\n",
    "    vol_25 = df['Close'].rolling(window=25).std().round(3)\n",
    "\n",
    "    new_cols = {\n",
    "        'vol_5': vol_5,\n",
    "        'vol_10': vol_10,\n",
    "        'vol_25': vol_25,\n",
    "        'Price_Vol_Ratio_5': (df['Close'] / vol_5).round(3),\n",
    "        'Price_Vol_Ratio_10': (df['Close'] / vol_10).round(3),\n",
    "        'Price_Vol_Ratio_25': (df['Close'] / vol_25).round(3)\n",
    "    }\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(new_cols, index=df.index)], axis=1)\n",
    "\n",
    "    # ================\n",
    "    # VIX External Data\n",
    "    # ================\n",
    "    vix_data = yf.Ticker(\"^VXN\").history(period='1d', start='2005-03-10')\n",
    "    # Resetting the index will turn the Date index into a column\n",
    "    vix_data = vix_data.reset_index()[['Date', 'Close', 'High', 'Low', 'Volume']]\n",
    "    # Convert the Date column to 'YYYY-MM-DD' format (if not already)\n",
    "    vix_data['Date'] = pd.to_datetime(vix_data['Date']).dt.strftime('%Y-%m-%d')\n",
    "    vix_data['VIX'] = vix_data['Close']\n",
    "\n",
    "    ['VIX_5_change', 'VIX_crossover', 'VIX_1_change']\n",
    "    # Convert the Date column to 'YYYY-MM-DD' format (if not already)\n",
    "    vix_data = vix_data.sort_index(ascending=True)\n",
    "    vix_data['VIX_rolling_std'] = vix_data['VIX'].rolling(window=5).std().round(1)\n",
    "    vix_data['VIX_short_ma'] = vix_data['VIX'].rolling(window=3).mean()\n",
    "    vix_data['VIX_long_ma'] = vix_data['VIX'].rolling(window=20).mean()\n",
    "    vix_data['VIX_crossover'] = np.where(vix_data['VIX_short_ma'] > vix_data['VIX_long_ma'], 1, -1)\n",
    "    vix_data['VIX_5_change'] = vix_data['VIX'].pct_change(periods=5).round(3)\n",
    "    vix_data['VIX_1_change'] = vix_data['VIX'].pct_change(periods=1).round(3)\n",
    "    vix_data['VIX_10_change'] = vix_data['VIX'].pct_change(periods=10).round(3)\n",
    "\n",
    "    df = pd.merge(df, vix_data[['Date', 'VIX', 'VIX_rolling_std', 'VIX_crossover', 'VIX_5_change', 'VIX_1_change', 'VIX_10_change']],\n",
    "                    on='Date', how='inner')\n",
    "\n",
    "    # ================\n",
    "    # Experimental Features\n",
    "    # ================\n",
    "    # CCI (Commodity Channel Index)\n",
    "    tp = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    ma = tp.rolling(window=14).mean()\n",
    "    md = tp.rolling(window=14).apply(lambda x: np.mean(np.abs(x - np.mean(x))))\n",
    "    df['CCI_14'] = ((tp - ma) / (0.015 * md)).round(3)\n",
    "\n",
    "    # Williams %R\n",
    "    highest_high = df['High'].rolling(window=14).max()\n",
    "    lowest_low = df['Low'].rolling(window=14).min()\n",
    "    df['Williams_%R_14'] = ((highest_high - df['Close']) / (highest_high - lowest_low) * -100).round(3)\n",
    "\n",
    "    # Z-scores\n",
    "    zs = [5, 10, 25, 50]\n",
    "    for z in zs:\n",
    "        df[f'Zscore_{z}'] = ((df['Close'] - df['Close'].rolling(z).mean()) / df['Close'].rolling(z).std()).round(3)\n",
    "        df[f'Zscore_{z}'] = df[f'Zscore_{z}'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # ================\n",
    "    # Convert All Non-Binary Variables into moving averages\n",
    "    # ================\n",
    "    windows   = [5, 10, 25, 50]\n",
    "    \n",
    "    temporal_targets = ['RSI_7', 'RSI_14', 'RSI_21', 'MACD', 'Signal_Line', 'CCI_14', 'Williams_%R_14', 'BB_Upper', 'BB_Lower', 'BB_Mid',\n",
    "    'OBV', 'OBV_ROC5', 'OBV_ROC10', 'OBV_Z5', 'OBV_Z10', 'CMF_20', 'ADL', 'VROC_5', 'Vol_Spike_10', 'Vol_Spike_20',\n",
    "    'Vol_Spike_40', 'Vol_Ratio_10', 'Vol_Ratio_25', 'Vol_Ratio_50', 'Vol_Ratio_100', 'ATR_7', 'ATR_14', 'ATR_21',\n",
    "    'vol_5', 'vol_10', 'vol_25', 'Price_Vol_Ratio_5', 'Price_Vol_Ratio_10', 'Price_Vol_Ratio_25', 'plus_DI', 'minus_DI',\n",
    "    'ADX', 'VIX', 'VIX_rolling_std', 'VIX_1_change', 'VIX_5_change', 'Zscore_5', 'Zscore_10', 'Zscore_25', 'Zscore_50']\n",
    "\n",
    "    # dictâ€‘comp to build every new Series, then concat once\n",
    "    # Simple Moving Average\n",
    "    new_cols = {\n",
    "        f'{var}_MA{w}': df[var].rolling(w).mean().div(df[var])\n",
    "        for w in windows\n",
    "        for var in temporal_targets\n",
    "    }\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(new_cols, index=df.index)], axis=1)  # one write\n",
    "\n",
    "    # Exponential Moving Average\n",
    "    new_cols = {\n",
    "        f'{var}_EA{w}': df[var].ewm(span=w, adjust=False).mean().div(df[var])\n",
    "        for w in windows\n",
    "        for var in temporal_targets\n",
    "    }\n",
    "    \n",
    "    df = pd.concat([df, pd.DataFrame(new_cols, index=df.index)], axis=1)  # one write\n",
    "\n",
    "    return df\n",
    "\n",
    "def regime(df_sma_returns): \n",
    "    \n",
    "    df_sma = df_sma_returns.copy()\n",
    "\n",
    "    # Daily Returns\n",
    "    df_sma[\"Daily Return\"] = df_sma[\"Close\"].pct_change()\n",
    "    # Intraday Volatility: High-Low Percentage Range\n",
    "    df_sma[\"Intraday Change\"] = (df_sma[\"High\"] - df_sma[\"Low\"]) / df_sma[\"Low\"]\n",
    "    df_sma[\"Intraday Change\"] *= np.sign(df_sma[\"Daily Return\"])\n",
    "    df_sma[\"Intraday Volatility\"] = df_sma[\"Intraday Change\"].rolling(window=10).std()\n",
    "\n",
    "    # === 3. Liquidity Score Calculation ===\n",
    "    # Calculate VWAP\n",
    "    df_sma[\"Typical Price\"] = (df_sma[\"High\"] + df_sma[\"Low\"] + df_sma[\"Close\"]) / 3\n",
    "    # Rolling 252-day median values for normalization\n",
    "    df_sma[\"Median Volume\"] = df_sma[\"Volume\"].rolling(window=50).median()\n",
    "    df_sma[\"Median Close\"] = df_sma[\"Close\"].rolling(window=50).median()\n",
    "\n",
    "    # Liquidity Score (using your formula)\n",
    "    df_sma[\"Liquidity Score\"] = (df_sma[\"Volume\"] * df_sma[\"Typical Price\"]) / (df_sma[\"Median Volume\"] * df_sma[\"Median Close\"])\n",
    "    df_sma['Regime'] = (df_sma[\"Liquidity Score\"] * (df_sma[\"Intraday Volatility\"] * 100)).round(2)\n",
    "\n",
    "    return df_sma\n",
    "\n",
    "def final_df(ticker, returns, lb):\n",
    "\n",
    "    # Define the ticker symbol\n",
    "    tickerSymbol = ticker\n",
    "\n",
    "    # Get data on this ticker\n",
    "    tickerData = yf.Ticker(tickerSymbol)\n",
    "    start_date = (datetime.today() - relativedelta(years=lb)).strftime('%Y-%m-%d')\n",
    "\n",
    "    tickerDf = tickerData.history(period='1d', start=start_date)\n",
    "    \n",
    "    # Resetting the index will turn the Date index into a column\n",
    "    df_sma = tickerDf.reset_index()[['Date', 'Close', 'High', 'Low', 'Volume']]\n",
    "\n",
    "    # Convert the Date column to 'YYYY-MM-DD' format (if not already)\n",
    "    df_sma['Date'] = pd.to_datetime(df_sma['Date']).dt.strftime('%Y-%m-%d')\n",
    "    #df_sma = pd.concat([df_sma, pd.DataFrame([main_row])], ignore_index=True)\n",
    "\n",
    "    df_sma = generate_all_features(df_sma)\n",
    "    df_sma = regime(df_sma)\n",
    "    p40, p80 = np.percentile(df_sma['Regime'].dropna(), [40, 80])\n",
    "    df_sma['Regime_Category'] = np.where(\n",
    "        df_sma['Regime'] < p40, 0,\n",
    "        np.where(df_sma['Regime'] > p80, 2, 1)\n",
    "    )\n",
    "\n",
    "    df_sma = df_sma.dropna()\n",
    "    df_sma_clean = df_sma.copy()\n",
    "    df_sma_clean = pd.DataFrame(df_sma_clean)\n",
    "    df_sma_clean = df_sma_clean.sort_index(ascending=True)\n",
    "\n",
    "    def add_column_based_on_future_value(df, days):\n",
    "        future_return = (df['Close'].shift(-days) - df['Close']) / df['Close']\n",
    "\n",
    "        if days >= 15:\n",
    "            df[f'Return_{days}'] = np.where(\n",
    "                future_return > 0.00, 1,\n",
    "                np.where(future_return < -0.00, 0, np.nan)\n",
    "            )\n",
    "        else:\n",
    "            df[f'Return_{days}'] = (future_return > 0).astype(int)\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Apply return logic for each target horizon\n",
    "    for r in returns:\n",
    "        df_sma_returns = add_column_based_on_future_value(df_sma_clean, r)\n",
    "\n",
    "    df_sma_returns = df_sma_returns.sort_index(ascending=False)\n",
    "\n",
    "    return df_sma_returns\n",
    "\n",
    "# Create an empty list to accumulate records\n",
    "def record_validation_metrics(metrics, arch, horizon, model_name):\n",
    "    \"\"\"\n",
    "    Convert metrics dictionary to DataFrame row format and store it.\n",
    "    \"\"\"\n",
    "    for t, vals in metrics.items():\n",
    "        row = {\n",
    "            'arch': arch,\n",
    "            'threshold': t,\n",
    "            'ticker': 'QQQ',\n",
    "            'horizon': horizon,\n",
    "            'model': model_name,\n",
    "        }\n",
    "        row.update(vals)\n",
    "        validation_results.append(row)\n",
    "\n",
    "def optimize_tests(df_indicators, df_predict, thresh, opt, depth, scale_pos_weight, min_child_weight, r, name, arch, date, return_metrics=False): \n",
    "\n",
    "    def train_and_evaluate(model, param_grid, X_train, X_test, y_train, y_test, opt, thresh):\n",
    "        \n",
    "        # Create a Stratified K-Fold object\n",
    "        stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        # Perform Random Search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid,  # Corrected from param_grid to param_distributions\n",
    "            scoring=opt,\n",
    "            cv=stratified_kfold,\n",
    "            n_jobs=-1,\n",
    "            n_iter=50,  # Adjust this based on how many random samples you want to try\n",
    "            random_state=42  # Ensures reproducibility\n",
    "        )\n",
    "\n",
    "        random_search.fit(X_train, y_train)\n",
    "        best_model = random_search.best_estimator_\n",
    "        # Predict probabilities\n",
    "        y_prob = best_model.predict_proba(X_test)\n",
    "\n",
    "        # Evaluate metrics for each threshold\n",
    "        metrics = {}\n",
    "        for t in thresh:\n",
    "            y_pred_thresh = (y_prob[:, 1] > t).astype(int)\n",
    "            y_pred_thresh[y_prob[:, 0] > t] = 0\n",
    "            filtered_indices = (y_prob[:, 1] > t) | (y_prob[:, 0] > t)\n",
    "\n",
    "            if filtered_indices.sum() > 0:\n",
    "                y_test_valid = y_test[filtered_indices]\n",
    "                y_pred_valid = y_pred_thresh[filtered_indices]\n",
    "                metrics[t] = {\n",
    "                    'PosF1': round(f1_score(y_test_valid, y_pred_valid), 3),\n",
    "                    'NegF1': round((2*round(recall_score(y_test_valid, y_pred_valid, pos_label=0), 3)*round(precision_score(y_test_valid, y_pred_valid, pos_label=0), 3))/(round(recall_score(y_test_valid, y_pred_valid, pos_label=0), 3) + round(precision_score(y_test_valid, y_pred_valid, pos_label=0), 3)),3),\n",
    "                    \"PosID'd\": round(recall_score(y_test_valid, y_pred_valid, pos_label=1), 3),  # % of total positives identified\n",
    "                    'PosPrec': round(precision_score(y_test_valid, y_pred_valid, pos_label=1), 3),  # % of positive predictions that were actually positive\n",
    "                    \"NegID'd\": round(recall_score(y_test_valid, y_pred_valid, pos_label=0), 3),  # % of total negatives identified\n",
    "                    'NegPrec': round(precision_score(y_test_valid, y_pred_valid, pos_label=0), 3),  # % of negative predictions that were actually negative\n",
    "                    'PosCnt': sum(y_pred_valid == 1),\n",
    "                    'NegCnt': sum(y_pred_valid == 0),\n",
    "                }\n",
    "            else:\n",
    "                metrics[t] = {'F1': 0, 'BalAcc': 0, 'PosAcc': 0, 'NegAcc': 0, 'PosCnt': 0, 'NegCnt': 0}\n",
    "\n",
    "        del random_search\n",
    "        gc.collect()\n",
    "\n",
    "        return metrics, best_model\n",
    "\n",
    "    if arch == 'shallow':\n",
    "        # Shallow\n",
    "        xgboost_hyperparameters = {\n",
    "            'scale_pos_weight': [scale_pos_weight],\n",
    "            'n_estimators': [200, 300],\n",
    "            'max_depth': [5, 7], \n",
    "            'learning_rate': [0.01],\n",
    "            'subsample': [0.65],\n",
    "            'colsample_bytree': [0.6], \n",
    "            'gamma': [0.2, 0.4],\n",
    "            'alpha': [0.1, 1], \n",
    "            'lambda': [1, 2, 5], \n",
    "            'min_child_weight': [12, 15]\n",
    "        }\n",
    "\n",
    "    elif arch == 'moderate':\n",
    "\n",
    "        # Moderate\n",
    "        xgboost_hyperparameters = {\n",
    "            'scale_pos_weight': [scale_pos_weight],\n",
    "            'n_estimators': [300, 400],\n",
    "            'max_depth': [7, 9], \n",
    "            'learning_rate': [0.01],\n",
    "            'subsample': [0.65, .75],\n",
    "            'colsample_bytree': [0.6, 0.7], \n",
    "            'gamma': [0.2, 0.3],\n",
    "            'alpha': [0.1, 1], \n",
    "            'lambda': [1, 2, 5], \n",
    "            'min_child_weight': [9, 11]\n",
    "        }\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Deep\n",
    "        xgboost_hyperparameters = {\n",
    "            'scale_pos_weight': [scale_pos_weight],\n",
    "            'n_estimators': [300, 400, 500],\n",
    "            'max_depth': [8, 10, 12], \n",
    "            'learning_rate': [0.01],\n",
    "            'subsample': [0.75, .85],\n",
    "            'colsample_bytree': [0.75, 0.85], \n",
    "            'gamma': [0.1, 0.2],\n",
    "            'alpha': [0.1, 1], \n",
    "            'lambda': [1, 2, 5], \n",
    "            'min_child_weight': [5, 7]\n",
    "        }\n",
    "\n",
    "    if date == 'lag':\n",
    "        p = 500\n",
    "        df_ind_rec = df_indicators.iloc[:p].copy()\n",
    "        df_pred_rec = df_predict.iloc[:p].copy()\n",
    "        df_indicators = df_indicators.iloc[p:]\n",
    "        df_predict = df_predict.iloc[p:]\n",
    "\n",
    "        # Split data once\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_indicators, df_predict, test_size=0.1, random_state=None, shuffle=True)\n",
    "        X_test = pd.concat([X_test, df_ind_rec], axis=0)\n",
    "        y_test = pd.concat([y_test, df_pred_rec], axis=0)\n",
    "\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_indicators, df_predict, test_size=0.3, random_state=None, shuffle=True)\n",
    "\n",
    "    # Train and evaluate models\n",
    "    xg_metrics, best_xg_model = train_and_evaluate(XGBClassifier(random_state=42), xgboost_hyperparameters, X_train, X_test, y_train, y_test, opt, thresh)\n",
    "    #record_validation_metrics(xg_metrics, arch='shallow', horizon=r, model_name=name)\n",
    "    #savearch(best_xg_model, r, name, arch)\n",
    "    print_metrics(xg_metrics)\n",
    "\n",
    "    if return_metrics:\n",
    "        return xg_metrics, best_xg_model\n",
    "    else:\n",
    "        #print_metrics(xg_metrics)\n",
    "        return best_xg_model, X_train, X_test, y_train, y_test\n",
    "    \n",
    "def savenew(best_model, days, model, arch, ticker, date, lb):\n",
    "    \n",
    "    directory = f'./New_Models/Ensemble_{ticker}'  # Saves to the current working directory\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    if ticker == 'QQQ':\n",
    "    \n",
    "        if date == 'current':\n",
    "            # Save the best xgboost model\n",
    "            with open(os.path.join(directory, f'{model}_xgboost_{days}{arch}_{lb}.pkl'), 'wb') as file_object:\n",
    "                pickle.dump(best_model, file_object)\n",
    "        else:\n",
    "            with open(os.path.join(directory, f'{model}_xgboost_{days}{arch}_{lb}.pkl'), 'wb') as file_object:\n",
    "                pickle.dump(best_model, file_object)\n",
    "\n",
    "    else:\n",
    "        with open(os.path.join(directory, str(model)+'_xgboost_'+str(days)+str(arch)+'.pkl'), 'wb') as file_object:\n",
    "                pickle.dump(best_model, file_object)\n",
    "\n",
    "    print('Models Saved for '+(ticker)+'_'+str(model)+' '+str(days)+'_'+str(date)+' Returns')\n",
    "\n",
    "def print_metrics(metrics):\n",
    "        for thresh, metric_values in metrics.items():\n",
    "            print(f\"  Threshold {thresh}: {metric_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df(ticker='QQQ', returns=[5, 10, 20], lb=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close'].to_csv('close.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
