{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import warnings\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import (fbeta_score, accuracy_score, f1_score, \n",
    "                             confusion_matrix, balanced_accuracy_score, recall_score, matthews_corrcoef, precision_score)\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "import gc\n",
    "pd.options.mode.chained_assignment = None \n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"A worker stopped while some jobs were given to the executor\",\n",
    "    category=UserWarning,\n",
    "    module=\"joblib.externals.loky.process_executor\"\n",
    ")\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import math\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# For importing universal scripts\n",
    "import sys\n",
    "import os\n",
    "# Go up two levels from the subfolder\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from indicators_returns import final_df #Universal script for indicator set and actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to accumulate records\n",
    "def record_validation_metrics(metrics, arch, horizon, model_name):\n",
    "    \"\"\"\n",
    "    Convert metrics dictionary to DataFrame row format and store it.\n",
    "    \"\"\"\n",
    "    for t, vals in metrics.items():\n",
    "        row = {\n",
    "            'arch': arch,\n",
    "            'threshold': t,\n",
    "            'ticker': 'QQQ',\n",
    "            'horizon': horizon,\n",
    "            'model': model_name,\n",
    "        }\n",
    "        row.update(vals)\n",
    "        validation_results.append(row)\n",
    "\n",
    "def optimize_tests(df_indicators, df_predict, thresh, opt, depth, scale_pos_weight, min_child_weight, r, name, arch, date, return_metrics=False): \n",
    "\n",
    "    def train_and_evaluate(model, param_grid, X_train, X_test, y_train, y_test, opt, thresh):\n",
    "        \n",
    "        # Create a Stratified K-Fold object\n",
    "        stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        # Perform Random Search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid,  # Corrected from param_grid to param_distributions\n",
    "            scoring=opt,\n",
    "            cv=stratified_kfold,\n",
    "            n_jobs=-1,\n",
    "            n_iter=50,  # Adjust this based on how many random samples you want to try\n",
    "            random_state=42  # Ensures reproducibility\n",
    "        )\n",
    "\n",
    "        random_search.fit(X_train, y_train)\n",
    "        best_model = random_search.best_estimator_\n",
    "        # Predict probabilities\n",
    "        y_prob = best_model.predict_proba(X_test)\n",
    "\n",
    "        # Evaluate metrics for each threshold\n",
    "        metrics = {}\n",
    "        for t in thresh:\n",
    "            y_pred_thresh = (y_prob[:, 1] > t).astype(int)\n",
    "            y_pred_thresh[y_prob[:, 0] > t] = 0\n",
    "            filtered_indices = (y_prob[:, 1] > t) | (y_prob[:, 0] > t)\n",
    "\n",
    "            if filtered_indices.sum() > 0:\n",
    "                y_test_valid = y_test[filtered_indices]\n",
    "                y_pred_valid = y_pred_thresh[filtered_indices]\n",
    "                metrics[t] = {\n",
    "                    'PosF1': round(f1_score(y_test_valid, y_pred_valid), 3),\n",
    "                    'NegF1': round((2*round(recall_score(y_test_valid, y_pred_valid, pos_label=0), 3)*round(precision_score(y_test_valid, y_pred_valid, pos_label=0), 3))/(round(recall_score(y_test_valid, y_pred_valid, pos_label=0), 3) + round(precision_score(y_test_valid, y_pred_valid, pos_label=0), 3)),3),\n",
    "                    \"PosID'd\": round(recall_score(y_test_valid, y_pred_valid, pos_label=1), 3),  # % of total positives identified\n",
    "                    'PosPrec': round(precision_score(y_test_valid, y_pred_valid, pos_label=1), 3),  # % of positive predictions that were actually positive\n",
    "                    \"NegID'd\": round(recall_score(y_test_valid, y_pred_valid, pos_label=0), 3),  # % of total negatives identified\n",
    "                    'NegPrec': round(precision_score(y_test_valid, y_pred_valid, pos_label=0), 3),  # % of negative predictions that were actually negative\n",
    "                    'PosCnt': sum(y_pred_valid == 1),\n",
    "                    'NegCnt': sum(y_pred_valid == 0),\n",
    "                }\n",
    "            else:\n",
    "                metrics[t] = {'F1': 0, 'BalAcc': 0, 'PosAcc': 0, 'NegAcc': 0, 'PosCnt': 0, 'NegCnt': 0}\n",
    "\n",
    "        del random_search\n",
    "        gc.collect()\n",
    "\n",
    "        return metrics, best_model\n",
    "\n",
    "    if arch == 'shallow':\n",
    "        # Shallow\n",
    "        xgboost_hyperparameters = {\n",
    "            'scale_pos_weight': [scale_pos_weight],\n",
    "            'n_estimators': [200, 300],\n",
    "            'max_depth': [5, 7], \n",
    "            'learning_rate': [0.01],\n",
    "            'subsample': [0.65],\n",
    "            'colsample_bytree': [0.6], \n",
    "            'gamma': [0.2, 0.4],\n",
    "            'alpha': [0.1, 1], \n",
    "            'lambda': [1, 2, 5], \n",
    "            'min_child_weight': [12, 15]\n",
    "        }\n",
    "\n",
    "    elif arch == 'moderate':\n",
    "\n",
    "        # Moderate\n",
    "        xgboost_hyperparameters = {\n",
    "            'scale_pos_weight': [scale_pos_weight],\n",
    "            'n_estimators': [300, 400],\n",
    "            'max_depth': [7, 9], \n",
    "            'learning_rate': [0.01],\n",
    "            'subsample': [0.65, .75],\n",
    "            'colsample_bytree': [0.6, 0.7], \n",
    "            'gamma': [0.2, 0.3],\n",
    "            'alpha': [0.1, 1], \n",
    "            'lambda': [1, 2, 5], \n",
    "            'min_child_weight': [9, 11]\n",
    "        }\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Deep\n",
    "        xgboost_hyperparameters = {\n",
    "            'scale_pos_weight': [scale_pos_weight],\n",
    "            'n_estimators': [300, 400, 500],\n",
    "            'max_depth': [8, 10, 12], \n",
    "            'learning_rate': [0.01],\n",
    "            'subsample': [0.75, .85],\n",
    "            'colsample_bytree': [0.75, 0.85], \n",
    "            'gamma': [0.1, 0.2],\n",
    "            'alpha': [0.1, 1], \n",
    "            'lambda': [1, 2, 5], \n",
    "            'min_child_weight': [5, 7]\n",
    "        }\n",
    "\n",
    "    if date == 'lag':\n",
    "        p = 500\n",
    "        df_ind_rec = df_indicators.iloc[:p].copy()\n",
    "        df_pred_rec = df_predict.iloc[:p].copy()\n",
    "        df_indicators = df_indicators.iloc[p:]\n",
    "        df_predict = df_predict.iloc[p:]\n",
    "\n",
    "        # Split data once\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_indicators, df_predict, test_size=0.1, random_state=None, shuffle=True)\n",
    "        X_test = pd.concat([X_test, df_ind_rec], axis=0)\n",
    "        y_test = pd.concat([y_test, df_pred_rec], axis=0)\n",
    "\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_indicators, df_predict, test_size=0.3, random_state=None, shuffle=True)\n",
    "\n",
    "    # Train and evaluate models\n",
    "    xg_metrics, best_xg_model = train_and_evaluate(XGBClassifier(random_state=42), xgboost_hyperparameters, X_train, X_test, y_train, y_test, opt, thresh)\n",
    "    #record_validation_metrics(xg_metrics, arch='shallow', horizon=r, model_name=name)\n",
    "    #savearch(best_xg_model, r, name, arch)\n",
    "    print_metrics(xg_metrics)\n",
    "\n",
    "    if return_metrics:\n",
    "        return xg_metrics, best_xg_model\n",
    "    else:\n",
    "        #print_metrics(xg_metrics)\n",
    "        return best_xg_model, X_train, X_test, y_train, y_test\n",
    "    \n",
    "def savenew(best_model, days, model, arch, ticker, date, lb):\n",
    "    \n",
    "    directory = f'./New_Models/Ensemble_{ticker}'  # Saves to the current working directory\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    if ticker == 'QQQ':\n",
    "    \n",
    "        if date == 'current':\n",
    "            # Save the best xgboost model\n",
    "            with open(os.path.join(directory, f'{model}_xgboost_{days}{arch}_{lb}.pkl'), 'wb') as file_object:\n",
    "                pickle.dump(best_model, file_object)\n",
    "        else:\n",
    "            with open(os.path.join(directory, f'{model}_xgboost_{days}{arch}_{lb}.pkl'), 'wb') as file_object:\n",
    "                pickle.dump(best_model, file_object)\n",
    "\n",
    "    else:\n",
    "        with open(os.path.join(directory, str(model)+'_xgboost_'+str(days)+str(arch)+'.pkl'), 'wb') as file_object:\n",
    "                pickle.dump(best_model, file_object)\n",
    "\n",
    "    print('Models Saved for '+(ticker)+'_'+str(model)+' '+str(days)+'_'+str(date)+' Returns')\n",
    "\n",
    "def print_metrics(metrics):\n",
    "        for thresh, metric_values in metrics.items():\n",
    "            print(f\"  Threshold {thresh}: {metric_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'QQQ'\n",
    "returns = [5, 10, 20]\n",
    "lb = 10\n",
    "df = final_df(ticker, returns, lb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
