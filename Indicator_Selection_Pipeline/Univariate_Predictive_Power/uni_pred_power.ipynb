{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# For importing universal scripts\n",
    "import sys\n",
    "import os\n",
    "# Go up two levels from the subfolder\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from indicators_returns import final_df #Universal script for indicator set and actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to accumulate records\n",
    "def record_validation_metrics(metrics, arch, horizon, model_name):\n",
    "    \"\"\"\n",
    "    Convert metrics dictionary to DataFrame row format and store it.\n",
    "    \"\"\"\n",
    "    for t, vals in metrics.items():\n",
    "        row = {\n",
    "            'arch': arch,\n",
    "            'threshold': t,\n",
    "            'ticker': 'QQQ',\n",
    "            'horizon': horizon,\n",
    "            'model': model_name,\n",
    "        }\n",
    "        row.update(vals)\n",
    "        validation_results.append(row)\n",
    "\n",
    "def optimize_tests(df_indicators, df_predict, thresh, opt, depth, scale_pos_weight, min_child_weight, r, name, arch, date, return_metrics=False): \n",
    "\n",
    "    def train_and_evaluate(model, param_grid, X_train, X_test, y_train, y_test, opt, thresh):\n",
    "        \n",
    "        # Create a Stratified K-Fold object\n",
    "        stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        # Perform Random Search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid,  # Corrected from param_grid to param_distributions\n",
    "            scoring=opt,\n",
    "            cv=stratified_kfold,\n",
    "            n_jobs=-1,\n",
    "            n_iter=50,  # Adjust this based on how many random samples you want to try\n",
    "            random_state=42  # Ensures reproducibility\n",
    "        )\n",
    "\n",
    "        random_search.fit(X_train, y_train)\n",
    "        best_model = random_search.best_estimator_\n",
    "        # Predict probabilities\n",
    "        y_prob = best_model.predict_proba(X_test)\n",
    "\n",
    "        # Evaluate metrics for each threshold\n",
    "        metrics = {}\n",
    "        for t in thresh:\n",
    "            y_pred_thresh = (y_prob[:, 1] > t).astype(int)\n",
    "            y_pred_thresh[y_prob[:, 0] > t] = 0\n",
    "            filtered_indices = (y_prob[:, 1] > t) | (y_prob[:, 0] > t)\n",
    "\n",
    "            if filtered_indices.sum() > 0:\n",
    "                y_test_valid = y_test[filtered_indices]\n",
    "                y_pred_valid = y_pred_thresh[filtered_indices]\n",
    "                metrics[t] = {\n",
    "                    'PosF1': round(f1_score(y_test_valid, y_pred_valid), 3),\n",
    "                    'NegF1': round((2*round(recall_score(y_test_valid, y_pred_valid, pos_label=0), 3)*round(precision_score(y_test_valid, y_pred_valid, pos_label=0), 3))/(round(recall_score(y_test_valid, y_pred_valid, pos_label=0), 3) + round(precision_score(y_test_valid, y_pred_valid, pos_label=0), 3)),3),\n",
    "                    \"PosID'd\": round(recall_score(y_test_valid, y_pred_valid, pos_label=1), 3),  # % of total positives identified\n",
    "                    'PosPrec': round(precision_score(y_test_valid, y_pred_valid, pos_label=1), 3),  # % of positive predictions that were actually positive\n",
    "                    \"NegID'd\": round(recall_score(y_test_valid, y_pred_valid, pos_label=0), 3),  # % of total negatives identified\n",
    "                    'NegPrec': round(precision_score(y_test_valid, y_pred_valid, pos_label=0), 3),  # % of negative predictions that were actually negative\n",
    "                    'PosCnt': sum(y_pred_valid == 1),\n",
    "                    'NegCnt': sum(y_pred_valid == 0),\n",
    "                }\n",
    "            else:\n",
    "                metrics[t] = {'F1': 0, 'BalAcc': 0, 'PosAcc': 0, 'NegAcc': 0, 'PosCnt': 0, 'NegCnt': 0}\n",
    "\n",
    "        del random_search\n",
    "        gc.collect()\n",
    "\n",
    "        return metrics, best_model\n",
    "\n",
    "    if arch == 'shallow':\n",
    "        # Shallow\n",
    "        xgboost_hyperparameters = {\n",
    "            'scale_pos_weight': [scale_pos_weight],\n",
    "            'n_estimators': [200, 300],\n",
    "            'max_depth': [5, 7], \n",
    "            'learning_rate': [0.01],\n",
    "            'subsample': [0.65],\n",
    "            'colsample_bytree': [0.6], \n",
    "            'gamma': [0.2, 0.4],\n",
    "            'alpha': [0.1, 1], \n",
    "            'lambda': [1, 2, 5], \n",
    "            'min_child_weight': [12, 15]\n",
    "        }\n",
    "\n",
    "    elif arch == 'moderate':\n",
    "\n",
    "        # Moderate\n",
    "        xgboost_hyperparameters = {\n",
    "            'scale_pos_weight': [scale_pos_weight],\n",
    "            'n_estimators': [300, 400],\n",
    "            'max_depth': [7, 9], \n",
    "            'learning_rate': [0.01],\n",
    "            'subsample': [0.65, .75],\n",
    "            'colsample_bytree': [0.6, 0.7], \n",
    "            'gamma': [0.2, 0.3],\n",
    "            'alpha': [0.1, 1], \n",
    "            'lambda': [1, 2, 5], \n",
    "            'min_child_weight': [9, 11]\n",
    "        }\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Deep\n",
    "        xgboost_hyperparameters = {\n",
    "            'scale_pos_weight': [scale_pos_weight],\n",
    "            'n_estimators': [300, 400, 500],\n",
    "            'max_depth': [8, 10, 12], \n",
    "            'learning_rate': [0.01],\n",
    "            'subsample': [0.75, .85],\n",
    "            'colsample_bytree': [0.75, 0.85], \n",
    "            'gamma': [0.1, 0.2],\n",
    "            'alpha': [0.1, 1], \n",
    "            'lambda': [1, 2, 5], \n",
    "            'min_child_weight': [5, 7]\n",
    "        }\n",
    "\n",
    "    if date == 'lag':\n",
    "        p = 500\n",
    "        df_ind_rec = df_indicators.iloc[:p].copy()\n",
    "        df_pred_rec = df_predict.iloc[:p].copy()\n",
    "        df_indicators = df_indicators.iloc[p:]\n",
    "        df_predict = df_predict.iloc[p:]\n",
    "\n",
    "        # Split data once\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_indicators, df_predict, test_size=0.1, random_state=None, shuffle=True)\n",
    "        X_test = pd.concat([X_test, df_ind_rec], axis=0)\n",
    "        y_test = pd.concat([y_test, df_pred_rec], axis=0)\n",
    "\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_indicators, df_predict, test_size=0.3, random_state=None, shuffle=True)\n",
    "\n",
    "    # Train and evaluate models\n",
    "    xg_metrics, best_xg_model = train_and_evaluate(XGBClassifier(random_state=42), xgboost_hyperparameters, X_train, X_test, y_train, y_test, opt, thresh)\n",
    "    #record_validation_metrics(xg_metrics, arch='shallow', horizon=r, model_name=name)\n",
    "    #savearch(best_xg_model, r, name, arch)\n",
    "    print_metrics(xg_metrics)\n",
    "\n",
    "    if return_metrics:\n",
    "        return xg_metrics, best_xg_model\n",
    "    else:\n",
    "        #print_metrics(xg_metrics)\n",
    "        return best_xg_model, X_train, X_test, y_train, y_test\n",
    "    \n",
    "def savenew(best_model, days, model, arch, ticker, date, lb):\n",
    "    \n",
    "    directory = f'./New_Models/Ensemble_{ticker}'  # Saves to the current working directory\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    if ticker == 'QQQ':\n",
    "    \n",
    "        if date == 'current':\n",
    "            # Save the best xgboost model\n",
    "            with open(os.path.join(directory, f'{model}_xgboost_{days}{arch}_{lb}.pkl'), 'wb') as file_object:\n",
    "                pickle.dump(best_model, file_object)\n",
    "        else:\n",
    "            with open(os.path.join(directory, f'{model}_xgboost_{days}{arch}_{lb}.pkl'), 'wb') as file_object:\n",
    "                pickle.dump(best_model, file_object)\n",
    "\n",
    "    else:\n",
    "        with open(os.path.join(directory, str(model)+'_xgboost_'+str(days)+str(arch)+'.pkl'), 'wb') as file_object:\n",
    "                pickle.dump(best_model, file_object)\n",
    "\n",
    "    print('Models Saved for '+(ticker)+'_'+str(model)+' '+str(days)+'_'+str(date)+' Returns')\n",
    "\n",
    "def print_metrics(metrics):\n",
    "        for thresh, metric_values in metrics.items():\n",
    "            print(f\"  Threshold {thresh}: {metric_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'QQQ'\n",
    "returns = [5, 10, 20]\n",
    "lb = 20\n",
    "df = final_df(ticker, returns, lb)\n",
    "df = df.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Univariate Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def evaluate_indicator_randomsplit(indicator_series, target_series, test_size=0.3, random_state=42):\n",
    "\n",
    "    \"\"\"Train logistic regression on a single indicator vs binary target. Return stats.\"\"\"\n",
    "    X = indicator_series.values.reshape(-1, 1)\n",
    "    y = target_series.values\n",
    "\n",
    "    # Drop NA\n",
    "    mask = ~np.isnan(X.flatten()) & ~np.isnan(y)\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    if len(np.unique(y)) < 2 or len(y) < 100:\n",
    "        return None  # not enough signal or samples\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    return {\n",
    "        'auc_rs': roc_auc_score(y_test, y_proba),\n",
    "        'log_loss_rs': log_loss(y_test, y_proba),\n",
    "        'coef_rs': model.coef_[0][0]\n",
    "    }\n",
    "\n",
    "def evaluate_indicator_recent500(indicator_series, target_series, p=500):\n",
    "\n",
    "    \"\"\"Train logistic regression on a single indicator vs binary target. Return stats.\"\"\"\n",
    "    X = indicator_series.values.reshape(-1, 1)\n",
    "    y = target_series.values\n",
    "\n",
    "    # Drop NA\n",
    "    mask = ~np.isnan(X.flatten()) & ~np.isnan(y)\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    if len(np.unique(y)) < 2 or len(y) < 100:\n",
    "        return None  # not enough signal or samples\n",
    "    \n",
    "    X_test = X[:p].copy()\n",
    "    y_test = y[:p].copy()\n",
    "    X_train = X[p:].copy()\n",
    "    y_train = y[p:].copy()\n",
    "\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    return {\n",
    "        'auc_ts': roc_auc_score(y_test, y_proba),\n",
    "        'log_loss_ts': log_loss(y_test, y_proba),\n",
    "        'coef_ts': model.coef_[0][0]\n",
    "    }\n",
    "\n",
    "r = 5\n",
    "df_test = df.copy()\n",
    "return_col = f'Return_{r}'\n",
    "indicator_cols = [col for col in df_test.columns \n",
    "                  if col not in ['Date', 'Close', 'High', 'Low', 'Volume']\n",
    "                  and not col.startswith('Return')\n",
    "]\n",
    "# Results for random split\n",
    "results_rs = []\n",
    "\n",
    "for col in indicator_cols:\n",
    "    df_eval = df.iloc[r:].copy() # Remove records without actuals\n",
    "    result = evaluate_indicator_randomsplit(df_eval[col], df_eval[return_col])\n",
    "    if result:\n",
    "        result['indicator'] = col\n",
    "        results_rs.append(result)\n",
    "\n",
    "logistic_results_rs = pd.DataFrame(results_rs)\n",
    "\n",
    "# Results for temporal split\n",
    "results_ts = []\n",
    "\n",
    "for col in indicator_cols:\n",
    "    df_eval = df.iloc[r:].copy() # Remove records without actuals\n",
    "    result = evaluate_indicator_recent500(df_eval[col], df_eval[return_col])\n",
    "    if result:\n",
    "        result['indicator'] = col\n",
    "        results_ts.append(result)\n",
    "\n",
    "logistic_results_ts = pd.DataFrame(results_ts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
